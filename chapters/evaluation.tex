In this chapter, we test the capabilities of  \demo{} as a framewor for
evalutating pub/sub systems. We extend the evaluation of PolderCast and
Scribe found in~\cite{Setty:2012} on a set of topology metrics. These
metrics are afforded to us for ``free'' by the Gephi framework through
the Statistics API included in the \emph{Gephi Toolkit}.

\section{\demo as a Framework for Evaluating Pub/Sub Systems}
\label{sec:viz_eval}

Although the plots seen in Chapter~\ref{ch:vizpub} are easy to produce
in Gephi, they are not very flexible. For example, it would be useful to
be able to superimpose two plots on top of each other in order to
effectively compare them. Therefore, in addition to outputting a \gexf
file which can be used to produce visualizations and plots in Gephi, The
collector is also able to generate \csv files which can be used to plot
a time series of metrics such as degree, clustering coefficient and
centralities. Each time point in the time series will represent a
\emph{Reporter Interval}. Although the Data Laboratory component in
Gephi is able to output such \csv files, it is much more convenient to
output them directly from the Collector, as opening the Gephi GUI-client
for the sole purpose of producing such files manually is more time
consuming, especially on older hardware, or machines without a dedicated
graphics card. The Collector is able to do this trough the Gephi
Toolkit, which provides an API for the major components of Gephi. Which
overlay properties to output is configurable in the Collector.
Currently, the supported metrics that can be exported to \csv files by
the Collector include:

\begin{itemize}
    \item Undirected Degree
    \item In-Degree
    \item Out-Degree
    \item Clustering Coefficient
    \item Betweenness Centrality
    \item Closeness Centrality
    \item Eccentricity Centrality
    \item Topic Diameter
    \item Size of Network
\end{itemize}

This grants researchers and developers of pub/sub protocols who wish to evaluate
the system in question immediate access to several metrics. They do not need to
reimplement algorithms for the metric calculations themselves. All that is
required is to implement the \emph{reporter interface}.

The plots produced aim at resembling the plots produced by the
Statistics Component in Gephi. However, due to using an external tool
for plotting, we can superimpose several plots on top of each other, as
well as adjust the format and layout of the plots. Also, it should be
noted that the output file format of the Gephi plots are non-vector
image files, which is not suitable for printing. The ability choose the
image format of the plots is another benefit over using the
standard plot output of Gephi.

\section{Experimental Restrictions}

As \demo{} is still is an early prototype state, there are restrictions
in terms the total number of nodes we can run as well as the number of
reporter intervals. This is due to the file sizes of temporary logs
grows linearly with the number of nodes, intervals and perhaps more
importantly: number of edges. This becomes problematic with pub/sub
systems which generate a high number of edges, such as PolderCast. The
Collector will also use a lot of memory when calculating custom metrics
due to loading these files in memory. Also, the Gephi Toolkit suffers
from performance issues due to an inefficient GEXF-parser. This means
that currently, \demo{} need a high amount of memory and disk space in
the order of several Gigabytes in order to operate properly.
Unfortunately, although we had access to the needed amount of memory, we
were restricted in terms of disk space. For this reason we restrict the
number of nodes and reporter intervals in our experiments.

The file sizes also grow linearly with the number of publication
messages. As the number of publication messages per interval can be in
the order of tens of thousands,  we evaluate only on topology metrics.
Also, due to time constraints we leave out the computational costly
\emph{topic diameter} metric, and leave this to future work.

We wish to improve the scalability of \demo{} in terms of file sizes in
the future, otherwise its usefulness for real deployed pub/sub systems
would be questionable at best. Luckily, there is a lot of low-hanging
fruits with regards to improving the performance in \demo{}. We describe
such future work in Chapter~\ref{ch:conclusion}.

\section{Experimental Setup}

We run PolderCast and Scribe in PeerNet using the simulation mode. The
Simulations consists of 1000 PeerNet cycles as well as 1000 reporter
intervals. We use workloads both from Facebook~\cite{facebook-eurosys09} and
Twitter~\cite{Kwak10www} in order to model subscriptions. As mentioned in
Chapter~\ref{ch:vizpub}, the Facebook data trace include 3 million user
profiles along with 28.3 million friend relations. The Twitter dataset
consists of 41.7 million distinct users and 1.47 billion
follow/followed relations.

The social relations in Facebook are reciprocal, which leads us to model
bidirectional subscriptions. More specifically, a Facebook user is
modeled as a topic. The friends list of the particular user profile
constitutes its subscription list. All of the entries in this list will
include the original user in their own lists of subscriptions.
Relationships in Twitter however, are unidirectional. When using the
Twitter trace, users are also modeled as topics, but here the list of
subscriptions are formed on the basis of the ``following'' list of the
particular  user profile. The entries in this list are not required to
follow back, therefore subscriptions are also unidirectional.

Churn is based on the Skype super-peer trace from~\cite{Guha:2006}, tracing 4000
nodes for 4 weeks, tracking their joining and leaving timestamps. We
scale churn to include the first day of this trace in order to not
introduce a churn rate which is unrealistically high. For latency
between node pairs, we use the King dataset found in~\cite{king}.

\section{Results}

Figure~\ref{fig:eval_directeddegree} shows directed degree in PolderCast
and Scribe. It confirms the much higher degree in PolderCast compared to
Scribe. It also a difference in average degree when using different
workloads. In particular, using the twitter workload seem to cause a
higher degree in both PolderCast and Scribe.  The difference is most
noticeable in Scribe, as the degree is quite low.  By using the
\emph{Statistics Component} in Gephi, we produce plots describing the
difference in number of edges in Scribe when using workloads from
Facebook and Twitter, seen in~\ref{fig:scribe_edges_face} and
Figure~\ref{fig:scribe_edges_twitter} respectively. It is clear that
using the workload from Twitter induces a much higher number of edges.
This might be indicative of a higher overlap in subscription interests,
increasing the number of tree structures being constructed. Such an
indication is also mentioned in~\cite{Setty:2012}. Furthermore, the
average degree are stable with regards to node churn, which might
indicate that the distribution of node degree does not diverge too much
from the mean. More specifically, there are few nodes with a very high
or a very low node degree. This decreases the likelihood of such a node
to leave the network due to churn, which would have a large impact on
the average value of node degree. The two plots seen in
Figure~\ref{fig:eval_directeddegree} also indicate that there is an even
balance between in-degree and out-degree of nodes, as they both have
similar values.

\begin{figure}[H]
    \centering
    \thisfloatpagestyle{empty}
        \input{eval_indegree.tex}
        \label{fig:eval_indegree}
        \input{eval_outdegree.tex}
        \label{fig:eval_outdegree}
    \caption{Avg. Directed Degrees of PolderCast and Scribe}
    \label{fig:eval_directeddegree}
\end{figure}

\begin{figure}[H]
    \centering
    \subfigure[Facebook] {
        \includegraphics[scale=0.3]{plots/scribe_edges_face}
    \label{fig:scribe_edges_face}
    }
    \subfigure[Twitter] {
        \includegraphics[scale=0.3]{plots/scribe_edges_twitter}
    \label{fig:scribe_edges_twitter}
    }
    \label{fig:scribe_edges}
    \caption{Plots describing the difference of number of edges in
        Scribe when using different subscription workloads.}
\end{figure}

Average Clustering coefficient is described in Figure~\ref{fig:eval_cc}.
The average is calculated by dividing the \emph{local clustering
    coefficient} of each node, and divide it by the number of nodes in
the overlay. As expected, PolderCast has a higher clustering
coefficient. However, it is worth noting that the clustering coefficient
of PolderCast seems relatively unaffected by using different workloads,
while in Scribe the clustering coefficient goes up when using the
Twitter~\cite{} dataset for subscriptions. Again, this is a natural
consequence of the much higher number of edges in Scribe when running
the Twitter workload.

Scalability issue.
\begin{figure}[H]
    \centering
    \input{eval_cc.tex}
    \caption{Avg. Clustering Coefficient of PolderCast and Scribe}
    \label{fig:eval_cc}
\end{figure}

\begin{figure}[H]
    \centering
    \input{eval_betweenness.tex}
    \caption{Avg. Betweenness Centrality of PolderCast and Scribe}
    \label{fig:eval_betweenness}
\end{figure}

\begin{figure}[H]
    \centering
    \input{eval_closeness.tex}
    \caption{Avg. Closeness Centrality of PolderCast and Scribe}
    \label{fig:eval_closeness}
\end{figure}

\begin{figure}[H]
    \centering
    \input{eval_eccentricity.tex}
    \caption{Avg. Eccentricity Centrality of PolderCast and Scribe}
    \label{fig:eval_eccentricity}
\end{figure}

