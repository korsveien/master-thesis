In this chapter, we test the capabilities of  \demo{} as a framework for
evalutating pub/sub systems. We extend the evaluation of PolderCast and
Scribe found in~\cite{Setty:2012} on a set of topology metrics by using
the Statistics API included in the \emph{Gephi Toolkit}.

\section{\demo as a Framework for Evaluating Pub/Sub Systems}
\label{sec:viz_eval}

Although the plots seen in Chapter~\ref{ch:vizpub} are easy to produce
in Gephi, they are not very flexible. For example, it would be useful to
be able to superimpose two plots on top of each other in order to
effectively compare them. Therefore, in addition to outputting a \gexf
file which can be used to produce visualizations and plots in Gephi, The
Collector is also able to generate \csv files which can be used to plot
a time series of metrics such as degree, clustering coefficient and
centralities. Each time point in these time series represents a
\emph{Reporter Interval}. Although the Data Laboratory component in
Gephi is able to output such \csv files, it is much more convenient to
output them directly from the Collector, as opening the Gephi GUI-client
for the sole purpose of producing such files manually is much more
cumbersome and time consuming, especially on older hardware, or machines
without a dedicated graphics card. The Collector is able to calculate
various metrics and output them to file by taking advantage of the Gephi
Toolkit, which provides an API for the major components of Gephi. Which
metrics to output is configurable in the Collector. Currently, the
supported metrics which can be exported to \csv files by the Collector
include:

\begin{itemize}
    \item Undirected Degree
    \item In-Degree
    \item Out-Degree
    \item Clustering Coefficient
    \item Betweenness Centrality
    \item Closeness Centrality
    \item Eccentricity Centrality
    \item Topic Diameter
    \item Size of Network
\end{itemize}

This capability of \demo{} grants researchers who wish to evaluate
any pub/sub system immediate access to several metrics. They do not need to
reimplement algorithms for the metric calculations. All that is
required of them is to implement the \emph{reporter interface}.

The plots produced in this chapter aim at resembling the plots produced
by the Statistics Component in Gephi. However, due to using an external
tool for plotting, we can superimpose several plots on top of each
other, as well as adjust the format and layout of the plots. Also, it
should be noted that the output file format of the Gephi plots are
non-vector image files, which is not very suitable for printing. The
ability to choose the image format of the plots is another benefit over
using the standard plot output of Gephi.

\section{Experimental Restrictions}

As \demo{} is still in an early prototype state, there are restrictions
in terms the total number of nodes we can run as well as the number of
reporter intervals. This is due to the file sizes of temporary logs
growing linearly with the number of nodes, the number of intervals and perhaps more
importantly: the number of edges. This becomes problematic with pub/sub
systems such as PolderCast, which generate a high number of edges. The
Collector will also consume a lot of memory when calculating custom metrics
due to the files note being streamed, but loaded directly into
memory. Also, the Gephi Toolkit suffers
from performance issues due to an inefficient GEXF-parser. This means
that currently, \demo{} need a high amount of memory and disk space in
the order of several Gigabytes in order to operate properly.
Unfortunately, although we had access to the needed amount of memory, we
were restricted in terms of disk space. For this reason we restrict the
number of nodes and reporter intervals in our experiments to 2000 nodes
and 1000 reporter intervals.

Another issue is the file sizes growing linearly with the number of
publication messages. As the number of publication messages per interval
can be in the order of tens of thousands,  we evaluate the systems on
topology metrics only. Also, due to time constraints we leave out the
computational costly \emph{topic diameter} metric, and leave this to
future work.

We wish to improve the scalability of \demo{} in terms of file sizes in
the future, otherwise its usefulness for real deployed pub/sub systems
would be questionable at best. We describe how to improve the
performance of \demo{} in Chapter~\ref{ch:conclusion}.

\section{Experimental Setup}

We run PolderCast and Scribe using the simulation mode in PeerNet. The
Simulations consists of 2000 nodes running 1000 PeerNet cycles as well as 1000 reporter
intervals. We use workloads both from Facebook~\cite{facebook-eurosys09} and
Twitter~\cite{Kwak10www} in order to model subscriptions. As mentioned in
Chapter~\ref{ch:vizpub}, the Facebook data trace include 3 million user
profiles along with 28.3 million friend relations. The Twitter dataset
consists of 41.7 million distinct users and 1.47 billion
follow/followed relations.

The social relations in Facebook are reciprocal, which leads us to model
bidirectional subscriptions. More specifically, a Facebook user is
modeled as a topic. The friends list of the particular user profile
constitutes its subscription list. All of the entries in this list will
include the original user in their own lists of subscriptions.
Relationships in Twitter however, are unidirectional. When using the
Twitter trace, users are also modeled as topics, but here the list of
subscriptions are formed on the basis of the ``following'' list of the
particular  user profile. The entries in this list are not required to
follow back, therefore subscriptions are also unidirectional.

Churn is based on the Skype super-peer trace from~\cite{Guha:2006}, tracing 4000
nodes for 4 weeks, tracking their joining and leaving timestamps. We
scale churn to include the first day of this trace in order to not
introduce a churn rate which is unrealistically high. For latency
between node pairs, we use the King dataset found in~\cite{king}.

\section{Results}

% \subsection{Degree}

\begin{figure}[Ht]
    \centering
    \subfigure[Facebook] {
        \includegraphics[scale=0.3]{plots/scribe_edges_face}
    \label{fig:scribe_edges_face}
    }
    \subfigure[Twitter] {
        \includegraphics[scale=0.3]{plots/scribe_edges_twitter}
    \label{fig:scribe_edges_twitter}
    }
    \label{fig:scribe_edges}
    \caption{Plots describing the difference of number of edges in
        Scribe when using different subscription workloads.}
\end{figure}

Figure~\ref{fig:eval_directeddegree} shows average directed degree in
PolderCast and Scribe. Averages are calculated by dividing the local
value of each node, and divide it by the number of nodes in the overlay
for the interval in question, i.e.\ the average is the \emph{arithmetic
    mean} for each reporter interval. This is the case for all metrics
described in this chapter. Figure~\ref{fig:eval_directeddegree} confirms
the much higher degree of PolderCast observed in
Chapter~\ref{ch:vizpub}. It also reveals a difference in average degree
when using different workloads. In particular, using the Twitter
workload seem to cause a higher degree in both PolderCast and Scribe.
The difference is most noticeable in Scribe, as the degree is quite low,
and thus the discrepancy appears to be larger.
By using the \emph{Statistics Component} in Gephi, we are able to
produce plots describing the difference in number of edges in Scribe
when using workloads from Facebook and Twitter, seen
in Figure~\ref{fig:scribe_edges_face} and Figure~\ref{fig:scribe_edges_twitter}
respectively. It is clear that using the workload from Twitter induces a
much higher number of edges. This might be indicative of a higher
overlap in subscription interests between nodes, increasing the number
of tree structures being constructed. Such an indication of subscription
overlap is also mentioned in~\cite{Setty:2012}. Furthermore, the average
degree seems to be stable in spite of node churn, which might indicate
that the distribution of node degree does not diverge too much from the
mean. More specifically, there are few nodes with a very high or a very
low node degree. This decreases the likelihood of such a node to leave
the network due to churn, which would have a large impact on the average
value of node degree. The two plots seen in
Figure~\ref{fig:eval_directeddegree} also indicate that there is an even
balance between in-degree and out-degree of nodes, as they both have
similar values.

\begin{figure}[H]
    \thisfloatpagestyle{empty}
    \centering
        \input{eval_indegree.tex}
        \label{fig:eval_indegree}
        \input{eval_outdegree.tex}
        \label{fig:eval_outdegree}
    \caption{Avg. Directed Degrees of PolderCast and Scribe}
    \label{fig:eval_directeddegree}
\end{figure}


% \subsection{Clustering Coefficient}

Average clustering coefficient is described in Figure~\ref{fig:eval_cc}.
As expected, PolderCast also has a higher average clustering
coefficient. Also here we can observe that the clustering coefficient of
PolderCast seems relatively unaffected by using different workloads,
while in Scribe the clustering coefficient increases when using the
Twitter dataset for subscriptions. Again, this is a natural consequence
of the much higher number of edges in Scribe when running the Twitter
workload. Also, the fact that PolderCast has a higher clustering
coefficient is hardly surprising, as the we know from
Chapter~\ref{ch:vizpub} that the overlay in PolderCast is more dense.

\begin{figure}[H]
    \centering
    \thisfloatpagestyle{empty}
    \input{eval_cc.tex}
    \caption{Avg. Clustering Coefficient of PolderCast and Scribe}
    \label{fig:eval_cc}
\end{figure}

%\subsection{Centralities}

Results of calculating centrality metrics can be seen in
Figure~\ref{fig:eval_betweenness}, Figure~\ref{fig:eval_closeness} and
Figure~\ref{fig:eval_eccentricity}. Again it can be observed in all
plots, that the values of the metric calculations in Scribe diverge
depending on the what workload is being used for subscriptions. No such
discrepancy can be seen in PolderCast.

Figure~\ref{fig:eval_betweenness} describes the average betweenness
centrality of nodes. Although PolderCast have a high betweenness, the
values do not seem to fluctuate too much depending on the workload. In
Scribe however, we see the mentioned discrepancy. Using the workload
from Twitter causes much higher values. Indeed, nodes in the Scribe
overlay have the highest average betweenness centralities when running
this workload. This could be an indication that Scribe does not scale
well in terms of betweenness as the number of edges increases. This
could impact the performance of the pub/sub system when running certain
workloads.

The average closeness centrality of nodes can be seen in
Figure~\ref{fig:eval_closeness}. Here Scribe populate the top values as
using both workloads, although the discrepancy of results depending on
the data set used is still higher than in PolderCast. This is to be
expected, as closeness is a measure between the distance of a node
$n$ and any other nodes in the system. Seeing that the overlay in
PolderCast is much more dense as well as having a shorter diameter, we
would expect nodes in PolderCast to have a lower closeness centrality as
well.

Figure~\ref{fig:eval_eccentricity} describes the average eccentricity
centrality of nodes. The result of running the eccentricity metric seems
to be comparable with the results in Figure~\ref{fig:eval_closeness}.
Again, running Scribe has the highest eccentricity when running the
Twitter workload. When running the Facebook workload however, Scribe
actually has a lower average eccentricity than compared to PolderCast.

The low number of edges and relative high number of
disconnected nodes seen in Scribe when running the workload from
Facebook might effect the results. It would be interesting to see what
is the cause of this. One possible reason might be the scale of the
experiment, as we could only run 2000 nodes for the purpose of this thesis. With more
time to spare, we would run further experiments at different scales in
order to test the effects. However, much work is needed before this is
possible, as \demo{} is still a young tool. We describe such future work
in Chapter~\ref{ch:conclusion} where we also address some of the
limitations of \demo{}

The low number of edges in Scribe, might also be caused by the amount of
churn in the system, as the dissemination structure in Scribe are , in
theory, brittle. Indeed, by running a short experiment without churn, we were
able to visually confirm that the overlay in Scribe was composed of only
one connected component by visually inspecting the graph in Gephi. This
might confirm in practice the brittleness of the overlay structures of Scribe,
but this issue needs further investigation. This investigation should include
adjusting the churn rate in order to see the effects of the overlays,
and perhaps compare the density of the graph, looking at the number of
edges formed as well as evaluating its connectivity.

\section{Summary}

It should be repeated that \demo{} is indeed a prototype, and there
might still be software bugs which affect the results. But the intention
of this chapter lies not only in producing results, but also test
\demo{} as a platform for evaluation of pub/sub systems. Although there
are many issues which still needs to be addressed, we believe the
potential for such a tool should be explored.

Our main concern when developing \demo{} was being able to visualize
pub/sub systems, but with this chapter we believe we provide some
examples to back up the claim that \demo{} can be used as a tool for
evaluation as well as visualization. We believe such a tool would be of
great benefit to the research community, and we hope the extendability
of Gephi through its ecosystem of plugins can encourage researchers to share
more of their implementation code.

% Currently, the collector does not provide us any information regarding
% the \emph{variance} or \emph{standard deviation} of the data being
% collected. However, using Gephi we can produce plots describing the
% distribution of values for a particular interval. As averages remain
% stable after the initial network spike, this should tell us something
% about how precise these averages are. In Figure~\ref{fig:betw_distr}
% we see such plots for the betweenness centralities in PolderCast and
% Scribe at interval 1000, running both the Facebook and Twitter
% datasets.

\begin{figure}[H]
    \centering
    \thisfloatpagestyle{empty}
    \input{eval_betweenness.tex}
    \caption{Avg. Betweenness Centrality of PolderCast and Scribe}
    \label{fig:eval_betweenness}
\end{figure}

% \begin{figure}[H]
% \vspace{-60pt}
%     \centering
%     \thisfloatpagestyle{empty}
%     \subfigure[PolderCast (Twitter)]{
%     \includegraphics[scale=0.3]{plots/polder_betw_distr_tw}
% }
%     \subfigure[Scribe (Twitter)]{
%     \includegraphics[scale=0.3]{plots/scribe_betw_distr_tw}
% }
%     \subfigure[PolderCast (Facebook)]{
%     \includegraphics[scale=0.3]{plots/polder_betw_distr_face}
% }
%     \subfigure[Scribe (Facebook)]{
%     \includegraphics[scale=0.3]{plots/scribe_betw_distr_face}
% }
%     \label{fig:betw_distr}
%     \caption{Plots showing distribution of betweenness in PolderCast and
%     Scribe at reporter interval 1000}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \thisfloatpagestyle{empty}
%     \subfigure[PolderCast (Twitter)]{
%     \includegraphics[scale=0.3]{plots/polder_clos_distr_tw}
% }
%     \subfigure[Scribe (Twitter)]{
%     \includegraphics[scale=0.3]{plots/scribe_clos_distr_tw}
% }
%     \subfigure[PolderCast (Facebook)]{
%     \includegraphics[scale=0.3]{plots/polder_clos_distr_face}
% }
%     \subfigure[Scribe (Facebook)]{
%     \includegraphics[scale=0.3]{plots/scribe_clos_distr_face}
% }
%     \label{fig:betw_distr}
%     \caption{Plots showing distribution of closeness in PolderCast and
%     Scribe at reporter interval 1000}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \thisfloatpagestyle{empty}
%     \subfigure[PolderCast (Twitter)]{
%     \includegraphics[scale=0.3]{plots/polder_ecc_distr_tw}
% }
%     \subfigure[Scribe (Twitter)]{
%     \includegraphics[scale=0.3]{plots/scribe_ecc_distr_tw}
% }
%     \subfigure[PolderCast (Facebook)]{
%     \includegraphics[scale=0.3]{plots/polder_ecc_distr_face}
% }
%     \subfigure[Scribe (Facebook)]{
%     \includegraphics[scale=0.3]{plots/scribe_ecc_distr_face}
% }
%     \label{fig:betw_distr}
%     \caption{Plots showing distribution of eccentricity in PolderCast and
%     Scribe at reporter interval 1000}
% \end{figure}

\begin{figure}[H]
    \centering
    \input{eval_closeness.tex}
    \caption{Avg. Closeness Centrality of PolderCast and Scribe}
    \label{fig:eval_closeness}
\end{figure}

\begin{figure}[H]
    \centering
    \input{eval_eccentricity.tex}
    \caption{Avg. Eccentricity Centrality of PolderCast and Scribe}
    \label{fig:eval_eccentricity}
\end{figure}

