\section{Thesis Work Description}
\subsection{Overview}
    The goal of this master thesis is to study and evaluate several topic-based
    pub/sub systems in order to compare them on a specific set of
    evaluation metrics. To the best of our knowledge, no such thorough
    comparison of topic-based pub/sub systems exist in literature. The following
    section will discuss the options that are available to us regarding
    evaluation methods, workloads and what evaluation metrics should be
    considered.

\subsection{Methodology}

    There are several options when it comes to methodology. Simulation
    is one such option, where the popular P2P overlay simulator PeerSim
    \cite{p2p09-peersim} seems like the most practical alternative.
    Some of the protocols previously discussed already have available
    implementations for PeerSim, while others would have to be
    implemented using the Java programming language. In particular, the
    protocols who would need implementation include Magnet, Vitis, StaN
    and SpiderCast which is only partially implemented. Table
    \ref{table:implementations} provides an overview over which
    protocols are already available, and which ones would need to be
    implemented from scratch.

    \begin{table}[h]
    \centering
     \begin{tabular}{| c | c |}
     \hline
     Protocol & Implementation available \\ \hline
     \hline
     Scribe \cite{Castro:2002} & \cmark \\ \hline
     Magnet \cite{Girdzijauskas:2010} & \xmark \\ \hline
     Bayeux \cite{Zhuang:2001} & \xmark \\ \hline
     Vitis \cite{Rahimian:2011} & \xmark \\ \hline
     SpiderCast \cite{Chockler:2007}* & \cmark \\ \hline
     StaN \cite{Matos:2010} & \xmark \\ \hline
     daMulticast \cite{Baehni:2004} & \cmark \\ \hline
     Quasar \cite{Wong:2008} & \xmark \\ \hline
     PolderCast \cite{Setty:2012} & \cmark \\ \hline
     ElastO \cite{Chen:2013} & \cmark \\ \hline
     \multicolumn{1}{l}{*Partially implemented} \\
     \end{tabular}
     \caption{Overview over existing Peersim implementations}
     \label{table:implementations}
    \end{table}


    There are also several considerations to make when planning the work
    forward. There might be issues with the existing PeerSim
    implementations which need to be resolved as well as issues with
    implementing protocols from scratch. A worst case scenario would be
    to discover in the end that a protocol simply cannot be implemented
    in PeerSim. If this is the case, there
    are several alternatives that should be discussed, as simply
    omitting the protocol in question, replacing it with another that
    represents a similar approach to topic-based pub/sub or
    implementing the protocol with the help of another simulator. In the
    last case, differences between simulators which might affect the
    comparison and evaluation of the protocol must be carefully
    considered. If no simulator is available then the only option would be
    to implement our own, this would require more time and effort which
    would affect the time available for other work. Whether or not to
    include such an effort would need to be discussed properly.

    Other considerations would be how to deal with input and output, as
    different protocols might require different formats in this regard.
    Using a standard format for input and creating a translation layer
    which adapts the input for each format seems like a reasonable
    approach. Some time should be allocated to developing
    tools and scripts aimed at creating a efficient and practical
    workflow when implementing and evaluating the protocols. This could
    be logging scripts where the level of detail might be specified, and
    runtime scripts for evaluation where the desired metrics, and
    protocols to evaluate can be defined. In general, we will aspire to
    automate as much of the process as possible.

    Which protocols to implement and evaluate will be an ongoing point
    of discussion as work progresses. It is important that the final set
    of protocols to evaluate is a good representation of the varied
    approaches to topic-based pub/sub that currently exist. This means
    that if two protocols are found to be too similar, we could decide
    to omit one of them. 
    
    We will also discuss whether or not to include an experiment
    involving deployment on the popular PlanetLab \cite{Chun:2003}
    testbed which currently consist of 1166 nodes at 551 sites. Such a
    large scale experiment would be a considerable effort, but would
    provide a more realistic environment for testing, providing further
    insight into the performance of these systems.

\subsection{Evaluation workloads}

    Several real-life traces and workloads are needed in order to
    perform a thorough evaluation. In particular, subscription workloads
    are needed in order to evaluate the different pub/sub systems. These
    workload consists of user profiles and the relationships between
    them. The users can be modeled as topics, which can be subscribed to
    by other users to form a friend relation. Note that some online
    social networks such as Twitter have unidirectional
    follower/followee relationships, where other social networks such as
    Facebook have bidirectional friend relations. This can be modeled as
    a single one-way subscription in the former case, while in the
    latter two publishers need to subscribe to each other. 
    
    Other type of datasets are also needed. Churn traces are required to
    model different failure scenarios. Other datasets might also be an
    option, such as datasets for modeling network latencies. Also,
    publication workloads in order to simulate publication events would
    be useful. However, there is a lack of such datasets, which means that
    these workloads might need to be created synthetically.

    On the question of which subscription workloads to choose, there are several
    alternatives that are available. One of these alternatives is a
    publicly available Facebook dataset \cite{facebook-eurosys09} consisting of 3
    million distinct users and 28.3 million social relations. There are
    also other Facebook datasets available such as 
     \cite{kurant11_magnifying} which consists of 1 million distinct
    users. Also, a Twitter dataset is available at \cite{Kwak10www}
    which consists of 41.7 million distinct user profiles and 1.47
    billion follower/followee relationships. Most likely, both datasets
    from Facebook and Twitter will be used in the evaluation.
    
    In order to evaluate the different systems during churn, real-world
    churn traces such as the Skype super-peer dataset from
    \cite{Guha:2006} is an option. This dataset tracks joining and
    leaving timestamps of 4000 nodes in the Skype network over a one
    month period. Finally, to simulate message latency, the King dataset
    \cite{king} would be a good option.

\subsection{Evaluation metrics}

    \subsubsection{Overview}

     There are several metrics that should be considered when evaluating the
     performance of these systems. This section will provide an overview of
     different metrics and their relevance to event dissemination in a P2P
     topic-based pub/sub systems. Discussing these metrics will be
     important, as they form one of the main contributions of this
     thesis. In general, a distinction can be made
     between evaluating the overlay itself or evaluating the efficiency of
     event dissemination i.e. the performance of message routing. 

\subsubsection{Overlay metrics}
    When evaluating the overlay,
    the following metrics are relevant:

    \begin{description}
        
    \item[Average node degree]\hfill\\
        It is essential to understand how the system scales with regard
        to the number of topics a node is interested in. For example, if the number of
        edges increases linearly with the subscription size of a node,
        scalability suffers. 

    \item[Maximum node degree]\hfill\\
        Also it might be interesting to measure the maximum node
        degree. As an example, having a relatively low average node
        degree while still having a rather high maximum node degree
        might reveal an unbalanced distribution of node degree in the
        graph.

    \end{description}

    The two metrics above could also be separated into measuring both
    in-degree and out-degree. A skewed distribution of directed edges
    would reveal an imbalance in the constructed overlay, or reveal
    vulnerable points in the system.

    \begin{description}

    \item[Betweenness centrality]\hfill\\
        Betweenness centrality is the number of times a particular node
        is found on the shortest path between two other nodes. A node
        with a big betweenness centrality may constitute both a
        vulnerable part of the graph as well as a bottleneck, as it
        might take part in a high number of event disseminations.

    \item[Topic diameter]\hfill\\
        This is the maximum number of hops between any two nodes that
        share interests, i.e. a measure of the diameter of a subgraph
        consisting only of nodes who registered their interest in the
        same topic. Having a low topic diameter is beneficial for
        disseminating events for topics.

    \item[Number of control messages]\hfill\\ 
        Some systems rely on
        control messages in order to maintain the overlay topology. For
        example in Scribe, where the multicast tree structures are
        maintained with periodic heartbeat messages. This constitutes an
        overhead both in communication and in overlay maintenance.

    \item[Clustering coefficient]\hfill\\ 
        This is the ratio of number of edges between neighbours of a node $n$ over
        the total number of possible edges between them. In simpler
        terms, how
        many of a nodes neighbours are connected to each other. A high
        clustering coefficient would indicate that the network has a
        higher risk of partitioning, as well as a risk of having a
        higher number of redundant message deliveries.

    \item[Partitionability]\hfill\\ 
        It could be useful to measure the minimal number of edges that
        need to be removed in order to partition the graph in order to
        evaluate connectivity. 

    \item[Expander graph metrics]\hfill\\ 
        Another measure of connectivity would be expander graph metrics
        such as vertex expansion and edge expansion, which measures the
        boundary of a subgraph $S$ that is no bigger than half the total
        number of nodes in the system. The vertex boundary is the set of
        vertices outside $S$ which has at least one neighbour in $S$.
        While the edge boundary would be the set of edges with exactly
        one endpoint in $S$.

    \end{description}

    Note that some of these metrics overlap with dissemination concerns
    such as number of control messages, which could be heartbeats
    used to maintain an overlay, or control messages that are part of
    the routing protocol. Regardless, it is possible to evaluate these
    two aspect of the protocols separately.

\subsubsection{Routing metrics}
    
    When evaluating the routing capabilities of the protocol, several
    metrics could be considered:

    \begin{description}
    \item[Hit-ratio during churn] \hfill\\ 
        It is essential to understand how the different systems respond to
        churn when disseminating events. If an overlay is robust, it should
        provide a high hit-ratio in the presence of realistic churn.
        Meaning that at very high percentage of subscribers receive the
        appropriate events. 

    \item[Average message delay] \hfill\\ 
        Counting the average number of nodes that are traversed in the
        overlay before an event reaches its target subscribers is
        helpful to understand the efficiency of the event dissemination.

    \item[Number of duplicate messages] \hfill\\
        If gossiping is used, subscribers are in danger of receiving the
        same event more than once. It would be interesting to measure
        the number of duplicates as it provides insight into the message overhead of the
        system.

    \item[Number of messages handled by a node per time unit] \hfill\\
        This metric would be one way to measure how load is distributed
        over the nodes in the system, where the time unit could be both
        seconds or cycles. Including the standard and mean deviation of
        this metric could tell whether or not the system is balanced in
        this regard.

    \end{description} 
    
    Note that several of these metrics are irrelevant for systems such
    as SpiderCast, StaN and ElastO who do not focus on event dissemination, but
    rather the construction and maintenance of the overlay itself.  We
    will look further into which set of metrics to measure, but some
    metrics should be essential. One of these metrics is hit-ratio
    during churn, as it provides basic information regarding both the
    robustness and reliability of the dissemination overlay. Another
    central issue to consider is the increase in node degree of a
    certain node compared to the number of topics that node is
    interested in. This would reveal any scalability issues regarding
    this aspect of the protocol. As mentioned in section 3, different
    systems have different approaches to maintaining a low node degree.
    Comparing the different protocols is an interesting study in what approach
    might be most suitable for online social networks.

    There are also other metrics to consider that is outside the scope
    of the overlay construction and the routing protocol. Resource
    consumption in terms of memory consumption, computational overhead
    and network utilisation would be such metrics. This will require a
    physical testbed of servers however, and such testing might be
    outside the scope of this master thesis.

    What set of metrics will be evaluated in the end will be an ongoing
    point of discussion. There might be several considerations that need
    to be made that are not clear to us at this point, but that might
    reveal themselves while working with the different implementations
    of the protocols. Also, the choice of which protocols to evaluate
    might influence the choice of which metrics should be considered, as
    some metrics might be less or more relevant depending on the
    protocols in question.


