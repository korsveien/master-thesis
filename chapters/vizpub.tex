In this chapter we describe \demo~\cite{korsveien2014vizpub}, a tool we
propose for visualizing the performance of overlay-based Pub/Sub
Systems. We presented poster and held a live demonstration of our system
at the ACM International Conference of Distributed Event Based Systems
(DEBS), held in Mumbai in May 2014, where it was awarded the price for
best poster/demo!

\section{System Overview}

To the best of our knowledge, \demo~is the first tool of its kind. The
tool is able to visualize the execution of any given distributed pub/sub
system step by step with respect to a set of performance metrics. Each
node in the system records records relevant data at selected
\emph{reporter intervals} during system execution. Our tool is then able to pull this data to a
single site, and compute various metrics at a system-wide scale. The
collected data from each interval is then collated into a single
gexf-file, which is interpreted by Gephi, which enable replay of system
execution offline.

Our tool supports two types of visualizations visualizations, the first is a
visualization of the overlay structure, and how it evolves over time.
The second type of animation is a hop-by-hop visualization of a single
publication message dissemination, where directed edges represent the
message dissemination path. We provide examples of both types of
visualizations later in this chapter.

There are several benefits to using a tool such as \demo. It enables
researchers and developers to gain a deeper insight into the overlay
structure as well as the publication process. It also has major benefits
as an educational tool, as it provides students with an visual
representation of both the structural evolution of the system, as well
as step-by-step animations of publication message disseminations. This
is useful in order to engage students, and facilitate deeper insight
into different pub/sub systems and their dissemination schemes. Such an
insight is also useful in order to identify potential weaknesses or
deployment anomalies of a given pub/sub systems. When developing the
tool, we encountered many scenarios where \demo~demonstrated its
usefulness. For example, when experimenting with PolderCast, we could
immediately verify that tree nodes were disconnected at the RINGS layer,
as seen in Figure~\ref{fig:pold_disc}.  Using our tool, we were able to
verify that this was caused by an artefact in the input workload where
the three nodes had no overlapping interest with any other node in the
system. We were then able to verify that the nodes were connected at the
CYCLON layer. We are not aware of any other tool or framework that would
allow such easy detection and validation of system behaviour.

\begin{figure}[h]
\includegraphics[width=\linewidth]{figures/disconnected-component-poldercast.pdf}
\caption{Visualization of disconnected component in the RINGS layer of PolderCast}
\label{fig:pold_disc}
\end{figure}

Another interesting use case for our tool is comparing different pub/sub systems and
protocols visually. Users may run the different systems using the same
workload, e.g.\ subscriptions and publications, and system parameters in
order to replay the execution and compare the different systems at
selected points in time. We include such comparisons in this chapter,
were we compare PolderCast and Scribe on a set of specific performance
metrics.

\section{System Architecture}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/arch}
\caption{Architecture diagram of \demo}
\label{fig:arch}
\end{figure}

The architecture of \demo~consists of three main components, all
depicted in Figure~\ref{fig:arch}: (1) \emph{Reporter}, (2)
\emph{Collector} and (3) \emph{Visualization Unit}.  The arrows seen in
Figure~\ref{fig:arch} depicts the flow of data in the architecture.
Each node in the executing system consists of a pub/sub protocol as well
as a ``Reporter''. The ``Reporter'' is the entity responsible for
providing the raw information required to compute various performance
metrics from the individual nodes participating in the overlay. This
information is pulled at regular intervals to a central site by the
``Collector'', which translates stores the information as a single
file in the GEXF-format. This file is then interpreted by the
``Visualization Unit'', which consists of a single machine running
the Gephi tool. The Collector is designed to perform the collection of
data while in online mode, while the computation, aggregation and
derivation of various metrics is performed in in offline mode. The
Visualization Unit always operate in offline mode, as it waits for the
final report to be collated into a GEXF-format for playback and
visualization.

The design of \demo~supports pub/sub systems that are deployed as
real distributed systems, as well as systems that are deployed in
simulation mode. This is due to the highly modular system architecture,
with strict separation of concerns, where the operation of both the
Collector and the Visualization Unit is separate from the reporting
. \demo~is designed to be a generic tool, where the only system specific
part of the architecture is the \emph{Reporter Interface} outlined in
Table~\ref{table:interface}. Any researcher or developer
who wants to use our framework only needs to provide an implementation
of this interface, which enables the Collector to retrieve the relevant
data from each individual node at the specified time points. These time
points are configurable, and all the information collected from the
reporter will in effect be the change of the system state from the last
time point with regards to the various performance metrics. We call
these time points \emph{reporter intervals}. The length of the intervals
is configurable, which provides the user with control over the
granularity of data collection, and thus the granularity of the
step-by-step replay of system execution performed in the Visualisation Unit. For example, if
running simulation using PeerNet, the user may determine whether or not
the Reporter Intervals should encompass several simulation cycles. Or,
in a real distributed pub/sub deployment scenario, the user can
determine the time delay between every reporter interval.

% ../tables/interface.tex
\input{tables/interface}

\subsection{Reporter}

The Reporter is responsible of capturing the relevant data

% ../tables/pubmessage.tex
\input{tables/pubmessage}

\subsection{Collector}

\subsection{Visualization Unit}

\section{Supported Metrics}

\section{Visualizations}
We implement a reporter interface both for PolderCast as well as Scribe.

\subsection{Overlay Structural}

\section{VizPub as a tool for evaluating pub/sub systems }

\section{Using Test-Driven Development}

Software Development Methodology is an active area of research
which is in part driven by the business needs of the private
sector\cite{janzen2005test}. One popular practice is so-called Test-Driven
Development (TDD). The promoters of TDD claims it increases
productivity and reduces the number of bugs and defects in the
code significantly~\cite{beck2003test}. Research
efforts performed at IBM~\cite{maximilien2003assessing} seems to
lend credibility to these claims. However, the use of TDD is not
prevalent in academia, and in~\cite{janzen2005test} they
recommend further research into the field in order to better
determine its effects.

Using TDD means writing tests before writing any code. There are
different types of test. \emph{Unit Tests} targets small,
independent pieces of code, typically methods within a single
module or component, while \emph{Integration Tests} aim to test
code across such modules and components in order to determine
how well they integrate with each other. In our work, we only
took advantage of Unit Tests where suitable using the
JUnit~\cite{junit} and Mockito~\cite{mockito} libraries.
We could also have benefited from a suite of integration tests,
as our implementation is heavily dependent on interoperating
components, as well as file and network IO\@. However, writing
these sort of tests would simply be too time consuming compared
to writing smaller unit tests.

The TDD approach to software development is best described through the
Red-Green-Refactor mantra, which is a central part of the
TDD-philosophy. It can be described through the following steps:

\begin{description}
    \item[Step 1:] Write a test that fails. (Red)
    \item[Step 2:] Make the test pass. (Green)
    \item[Step 3:] Refactor the code while making sure the test
        still passes. (Refactor)
\end{description}

In our experience this routine has been helpful when working
with our implementation code, as it enables us as developer to
refactor with confidence achieving more maintainable code and a
more thoughtful software design. Since we share our
implementation code with the research community by hosting it in
a open repository, any tool or method that helps us improve the
design and maintainability of our project is of great value to
us. Using TDD forced us to think more deeply about what
functionality to implement and how to structure and split the
problem domain into smaller function points. We believe that in
the end, following TDD where its suitable is beneficial to both
programmer productivity as well as programmer happiness. Also,
we are confident that this practice decreased the amount of
technical debt in our project, a problem we find to be commonplace in academia.
